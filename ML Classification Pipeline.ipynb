{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Classification Pipeline\n",
    "This code is self contained and explores the performance of a large number of ML algorithms to predict binary labels from data features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the necessary libraries and functions that we will use in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, PassiveAggressiveClassifier, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, IsolationForest, VotingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA, SparsePCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import Imputer\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load our data, say in csv format, and look at simple statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/...')\n",
    "\n",
    "print(dataset.describe())\n",
    "print(dataset.dtypes)\n",
    "print(dataset.columns.values)\n",
    "\n",
    "def get_data(target = 'label'):\n",
    "    dataset = pd.read_csv('/home/...')\n",
    "    #dataset = dataset.iloc[:500,:]\n",
    "    X = dataset.drop([target], axis=1, inplace=False)\n",
    "    y = (dataset[[target]])*1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting preliminary step to understand the relationships between features and labels is to fit a simple logistic regression model and explore its parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y           = get_data(target = 'label')\n",
    "logit = sm.Logit(y, X)\n",
    "\n",
    "# fit the model\n",
    "result = logit.fit()\n",
    "# summary report\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define the main loop to test our classification algorithms. Each model and cross-validated performance using a number of metrics is obtained with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Run_benchmark_classifiers(X, y, Model_list, Metrics_list, num_folds):\n",
    "    text_file = open(\"/home/...\", \"w\")  # Open a file to store results\n",
    "\n",
    "    ## Preprocessing step\n",
    "    # X        = preprocessing.scale(X) # scaling\n",
    "    X = preprocessing.normalize(X)  # normalization\n",
    "    # pipeline = Pipeline([('scaling', Normalizer()), ('pca', PCA(n_components=4))]) # create a pipeline\n",
    "    # pipeline = Pipeline([('scaling', Normalizer()), ('fast_ica', FastICA(n_components=4))])\n",
    "    # pipeline = Pipeline([('scaling', Normalizer()), ('sparse_pca', SparsePCA(n_components=4))])\n",
    "    # X        = pipeline.fit_transform(X) # In future versions split this between training and testing\n",
    "    # poly     = PolynomialFeatures(interaction_only=True)\n",
    "    # X        = poly.fit_transform(X)\n",
    "    # agglo    = FeatureAgglomeration(n_clusters=4)\n",
    "    # agglo.fit(X)\n",
    "    # X        = agglo.transform(X)\n",
    "    # RFF       = RBFSampler(gamma=1, random_state=1)\n",
    "    # X         = RFF.fit_transform(X)\n",
    "\n",
    "    for i in range(len(Model_list)):\n",
    "\n",
    "        # Create an instantiation of the selected model\n",
    "        if Model_list[i][0] == 'Logistic Regression':\n",
    "            model = LogisticRegression()\n",
    "        elif Model_list[i][0] == 'SGD':\n",
    "            model = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "        elif Model_list[i][0] == 'kNN':\n",
    "            model = KNeighborsClassifier()\n",
    "        elif Model_list[i][0] == 'Decision Tree':\n",
    "            model = tree.DecisionTreeClassifier()\n",
    "        elif Model_list[i][0] == 'Naive Bayes':\n",
    "            if Model_list[i][1] == 'Gaussian':\n",
    "                model = GaussianNB()\n",
    "            if Model_list[i][1] == 'Bernoulli':\n",
    "                model = BernoulliNB()\n",
    "            if Model_list[i][1] == 'Multinomial':\n",
    "                model = MultinomialNB()\n",
    "        elif Model_list[i][0] == 'SVM':\n",
    "            if Model_list[i][1] == 'Linear':\n",
    "                model = svm.LinearSVC()\n",
    "            if Model_list[i][1] == 'Kernel':\n",
    "                model = svm.SVC()\n",
    "        elif Model_list[i][0] == 'Random Forest':\n",
    "            model = RandomForestClassifier(n_estimators=20)\n",
    "        elif Model_list[i][0] == 'Extra Trees':\n",
    "            model = ExtraTreesClassifier(n_estimators=20)\n",
    "        elif Model_list[i][0] == 'LDA':\n",
    "            model = LinearDiscriminantAnalysis()\n",
    "        elif Model_list[i][0] == 'QDA':\n",
    "            model = QuadraticDiscriminantAnalysis()\n",
    "        elif Model_list[i][0] == 'Passive Agressive':\n",
    "            model = PassiveAggressiveClassifier()\n",
    "        elif Model_list[i][0] == 'AdaBoost':\n",
    "            model = AdaBoostClassifier()\n",
    "        elif Model_list[i][0] == 'Bagging':\n",
    "            model = BaggingClassifier()\n",
    "        elif Model_list[i][0] == 'Gradient Boosting':\n",
    "            model = GradientBoostingClassifier()\n",
    "        elif Model_list[i][0] == 'Isolation Forest':\n",
    "            model = IsolationForest()\n",
    "        elif Model_list[i][0] == 'XGBoost':\n",
    "            model = xgb.XGBClassifier()\n",
    "        elif Model_list[i][0] == 'LASSO':\n",
    "            model = Lasso()\n",
    "        elif Model_list[i][0] == 'Dropout':\n",
    "            layers = [Layer(\"Rectifier\", units=100), Layer(\"Rectifier\", units=100), Layer(\"Softmax\")]\n",
    "            model = Classifier(layers, dropout_rate=0.5)\n",
    "        elif Model_list[i][0] == 'MLP':\n",
    "            model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=Model_list[i][1], random_state=1)\n",
    "        elif Model_list[i][0] == 'AutoML':\n",
    "            if Model_list[i][1] == 'Evolutionary':\n",
    "                model = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "            if Model_list[i][1] == 'Bayesian':\n",
    "                model = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "\n",
    "        # Compute the set of metrics via K-fold cross validation\n",
    "        # -------------------------------------------------------\n",
    "        if Model_list[i][0] == 'Naive Bayes':\n",
    "            text_file.write(\"**Performance of %s %s:\" % (Model_list[i][1], Model_list[i][0]))\n",
    "            print(\"Performance of %s %s:\" % (Model_list[i][1], Model_list[i][0]))\n",
    "        else:\n",
    "            text_file.write(\"**Performance of %s:\" % Model_list[i][0])\n",
    "            print(\"Performance of %s:\" % Model_list[i][0])\n",
    "        for k in range(len(Metrics_list)):\n",
    "            scores_val = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=num_folds), scoring=Metrics_list[k])\n",
    "            if k == len(Metrics_list) - 1:\n",
    "                text_file.write(\n",
    "                    \"%s = %0.4f (+/- %0.4f)**\\n\" % (Metrics_list[k], scores_val.mean(), scores_val.std() * 2))\n",
    "                print(\"%s = %0.4f (+/- %0.4f)\\n\" % (Metrics_list[k], scores_val.mean(), scores_val.std() * 2))\n",
    "            else:\n",
    "                text_file.write(\"%s = %0.4f (+/- %0.4f),\" % (Metrics_list[k], scores_val.mean(), scores_val.std() * 2))\n",
    "                print(\"%s = %0.4f (+/- %0.4f)\" % (Metrics_list[k], scores_val.mean(), scores_val.std() * 2))\n",
    "    text_file.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the algorithms of interest, number of folds for cross-validation and metrics to compute performance. And RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_folds     = 10                                                        # number of crossvalidation folds\n",
    "Model_list    = [['Logistic Regression'],['SGD'],['kNN'],\n",
    "                 ['Decision Tree'],['Naive Bayes','Gaussian'],['Naive Bayes','Bernoulli'],\n",
    "                 ['Naive Bayes','Multinomial'],['SVM','Linear'],['SVM','Kernel'],['QDA'],\n",
    "                 ['Random Forest'], ['Extra Trees'],['LDA'],['Passive Agressive'],\n",
    "                 ['AdaBoost'],['Bagging'],\n",
    "                 ['Gradient Boosting'],['XGBoost'],['MLP',(100,100)]]     # List of models and parameters\n",
    "Metrics_list  = ['roc_auc','recall','precision','accuracy','average_precision']\n",
    "                                                                          # List of metrics for benchmark evaluation\n",
    "    \n",
    "\n",
    "# Run benchmarks in the model list\n",
    "#-------------------------------------------------------------------------\n",
    "Run_benchmark_classifiers(X,y,Model_list,Metrics_list,num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable importance\n",
    "Finally, consider variable importance computation by fitting a desired model with a single feature at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_importance(X,y):\n",
    "    scores = []\n",
    "    X = preprocessing.normalize(X)\n",
    "    #model = GradientBoostingClassifier()\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        #data = np.delete(X, i, axis=1)\n",
    "        data = X[:,i]\n",
    "        data = data[:,np.newaxis]\n",
    "        scores_val = cross_val_score(model, data, y, cv=StratifiedKFold(n_splits=3), scoring='roc_auc')\n",
    "        scores.append(scores_val.mean())\n",
    "        \n",
    "        #print(\"AUC for %s removed = %0.4f (+/- %0.4f)\" % (dataset.columns.values[4+i], scores_val.mean(), \n",
    "        #                                                    scores_val.std()))\n",
    "        print(\"AUC with %s only = %0.4f (+/- %0.4f)\" % (dataset.columns.values[4+i], scores_val.mean(), \n",
    "                                                            scores_val.std()))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y           = get_data(target = 'target')\n",
    "\n",
    "scores = variable_importance(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "669px",
    "left": "1485px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
